{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Inference_examples.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Machine Learning for Physical Sciences\n",
    "*pip install mlphys*\n",
    "\n",
    "Author: Sanjaya Lohani\n",
    "\n",
    "*Please report bugs at slohani@mlphys.com\n",
    "\n",
    "Papers:\n",
    "\n",
    "1.   Lohani, S., Lukens, J.M., Jones, D.E., Searles, T.A., Glasser, R.T. and Kirby, B.T., 2021. Improving application performance with biased distributions of quantum states. *Physical Review Research*, 3(4), p.043145. \n",
    "\n",
    "2.  Lohani, S., Searles, T. A., Kirby, B. T., & Glasser, R. T. (2021). On the Experimental Feasibility of Quantum State Reconstruction via Machine Learning. *IEEE Transactions on Quantum Engineering*, 2, 1â€“10. \n",
    "\n",
    "Collaborator: Joseph M. Lukens, Daniel E. Jones, Ryan T. Glasser, Thomas A. Searles, and Brian T. Kirby\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "to8czZ-hqNwv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11eJ2p0Ek8N9"
   },
   "outputs": [],
   "source": [
    "!pip install mlphys"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import mlphys.deepqis.simulator.distributions as dist\n",
    "import mlphys.deepqis.simulator.measurements as meas\n",
    "import mlphys.deepqis.utils.Alpha_Measure as find_alpha\n",
    "import mlphys.deepqis.utils.Concurrence_Measure as find_con\n",
    "import mlphys.deepqis.utils.Purity_Measure as find_pm\n",
    "import mlphys.deepqis.network.inference as inference\n",
    "import mlphys.deepqis.utils.Fidelity_Measure as fm\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "vilukIJNlMDT"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Generating 100 test sets \n",
    "# Random measurements = 1000. \n",
    "#\"\"\" NOTE: The Random measurements illustrate the use of a single basis chosen \n",
    "# randomly at a time, which is different from the term \"shots\" used in NISQ. \n",
    "#\"\"\"\n",
    "\n",
    "# 1. Bures and its tomography \n",
    "bures = dist.Bures(qs=2).sample_dm(100)\n",
    "tomo_bures,_ = meas.Random_Measurements(qs=2, n_meas=1000).tomography_data(bures)\n",
    "\n",
    "# 2. HS\n",
    "hs = dist.Hilbert_Schmidt(qs=2).sample_dm(100)\n",
    "tomo_hs,_ = meas.Random_Measurements(qs=2, n_meas=1000).tomography_data(hs)\n",
    "\n",
    "#3. Haar random pure states\n",
    "haar = dist.Haar_State(qs=2).sample_dm(100)\n",
    "tomo_haar,_ = meas.Random_Measurements(qs=2, n_meas=1000).tomography_data(haar)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tRVjYETlSyP",
    "outputId": "c63e7b42-9478-4c8b-c600-2943a03f391f"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| To accelerate the simulation, General Scheme Projector file is created in a new projectors_general folder.\n",
      "|Found projectors_general folder! Appending some files ...\n",
      "| To accelerate the simulation, General Scheme Projector file is created in a new projectors_general folder.\n",
      "|Found projectors_general folder! Appending some files ...\n",
      "| To accelerate the simulation, General Scheme Projector file is created in a new projectors_general folder.\n",
      "|Found projectors_general folder! Appending some files ...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### To demonstrate the efficacy of biased distributions, we have inculded only two alpha \n",
    "\n",
    "---\n",
    "\n",
    "values at the moment. $\\alpha = 0.1, 0.4$\n",
    "\n",
    "##### 1. Reconstructing States using a pre-trained model with MA at $\\alpha = 0.1$\n",
    "\n"
   ],
   "metadata": {
    "id": "I6BqIIoVtcRC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pred_bures, _ = inference.fit(tomo_bures, alpha=0.1)\n",
    "pred_hs, _ = inference.fit(tomo_hs, alpha=0.1)\n",
    "pred_haar, model = inference.fit(tomo_haar, alpha=0.1)\n",
    "\n",
    "print ('*'*100)\n",
    "\n",
    "_, fid_bures_av = fm.Fidelity_Metric(bures, pred_bures)\n",
    "print(\"Mean Fidelity for Bures: \", fid_bures_av)\n",
    "\n",
    "_, fid_hs_av = fm.Fidelity_Metric(hs, pred_hs)\n",
    "print(\"Mean Fidelity for HS: \", fid_hs_av)\n",
    "\n",
    "_, fid_haar_av = fm.Fidelity_Metric(haar, pred_haar)\n",
    "print(\"Mean Fidelity for Haar: \", fid_haar_av)\n",
    "print ('*'*100)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzi3T-YqnIR9",
    "outputId": "c3bf77d0-6709-45e7-bdb7-33ee01281d60"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "****************************************************************************************************\n",
      "Mean Fidelity for Bures:  tf.Tensor(0.908354743815255, shape=(), dtype=float64)\n",
      "Mean Fidelity for HS:  tf.Tensor(0.8860056817796851, shape=(), dtype=float64)\n",
      "Mean Fidelity for Haar:  tf.Tensor(0.9741458761737515, shape=(), dtype=float64)\n",
      "****************************************************************************************************\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2. Reconstructing States using a pre-trained model with MA at $\\alpha = 0.4$"
   ],
   "metadata": {
    "id": "tmWWrjUhAAkR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pred_bures, _ = inference.fit(tomo_bures, alpha=0.4)\n",
    "pred_hs, _ = inference.fit(tomo_hs, alpha=0.4)\n",
    "pred_haar, model = inference.fit(tomo_haar, alpha=0.4)\n",
    "\n",
    "print ('*'*100)\n",
    "\n",
    "_, fid_bures_av = fm.Fidelity_Metric(bures, pred_bures)\n",
    "print(\"Mean Fidelity for Bures: \", fid_bures_av)\n",
    "\n",
    "_, fid_hs_av = fm.Fidelity_Metric(hs, pred_hs)\n",
    "print(\"Mean Fidelity for HS: \", fid_hs_av)\n",
    "\n",
    "_, fid_haar_av = fm.Fidelity_Metric(haar, pred_haar)\n",
    "print(\"Mean Fidelity for Haar: \", fid_haar_av)\n",
    "print ('*'*100)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDkH9LzSBMGn",
    "outputId": "2e3d016a-636a-4e05-dc2b-6f9d94fce4f7"
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "****************************************************************************************************\n",
      "Mean Fidelity for Bures:  tf.Tensor(0.9454871408272887, shape=(), dtype=float64)\n",
      "Mean Fidelity for HS:  tf.Tensor(0.9442138892237649, shape=(), dtype=float64)\n",
      "Mean Fidelity for Haar:  tf.Tensor(0.9397807952668203, shape=(), dtype=float64)\n",
      "****************************************************************************************************\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View the model used. \n"
   ],
   "metadata": {
    "id": "5WhfUu4sDrkD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5WZTwja8FAu",
    "outputId": "213028da-8fe5-403a-cbd1-b8d1b2032552"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ARL_Nets\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Tomography_Measurements (Inp [(None, 6, 6, 1)]         0         \n",
      "_________________________________________________________________\n",
      "First_CONV (Conv2D)          (None, 6, 6, 64)          320       \n",
      "_________________________________________________________________\n",
      "First_MAXPOOL (MaxPooling2D) (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "Second_CONV (Conv2D)         (None, 3, 3, 64)          16448     \n",
      "_________________________________________________________________\n",
      "FLATTEN (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "First_DENSE (Dense)          (None, 3000)              1731000   \n",
      "_________________________________________________________________\n",
      "First_DROPOUT (Dropout)      (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "Sec_DENSE (Dense)            (None, 1200)              3601200   \n",
      "_________________________________________________________________\n",
      "Sec_DROPOUT (Dropout)        (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "Tau_Elements (Dense)         (None, 16)                19216     \n",
      "_________________________________________________________________\n",
      "Density_Matrix (PredictDensi (None, 4, 4)              0         \n",
      "=================================================================\n",
      "Total params: 5,368,184\n",
      "Trainable params: 5,368,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The tensorflow model can also be directly imported from the 'load' module. After that it can be used in fine tuning any other network-settings and training scenarios."
   ],
   "metadata": {
    "id": "qwhuTS4jBjMB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from mlphys.deepqis.utils import Extract_Net\n",
    "model_h5 = inference.load(alpha=0.4)\n",
    "model = tf.keras.models.load_model(model_h5, custom_objects={'ErrorNode':Extract_Net.ErrorNode, \n",
    "                                        'PredictDensityMatrix':Extract_Net.PredictDensityMatrix})\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOTLg8sDCclt",
    "outputId": "6099e76e-17b3-4ce7-d67a-fd2fbfd65248"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:Using a while_loop for converting SparseToDense\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"ARL_Nets\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Tomography_Measurements (Inp [(None, 6, 6, 1)]         0         \n",
      "_________________________________________________________________\n",
      "First_CONV (Conv2D)          (None, 6, 6, 64)          320       \n",
      "_________________________________________________________________\n",
      "First_MAXPOOL (MaxPooling2D) (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "Second_CONV (Conv2D)         (None, 3, 3, 64)          16448     \n",
      "_________________________________________________________________\n",
      "FLATTEN (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "First_DENSE (Dense)          (None, 3000)              1731000   \n",
      "_________________________________________________________________\n",
      "First_DROPOUT (Dropout)      (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "Sec_DENSE (Dense)            (None, 1200)              3601200   \n",
      "_________________________________________________________________\n",
      "Sec_DROPOUT (Dropout)        (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "Tau_Elements (Dense)         (None, 16)                19216     \n",
      "_________________________________________________________________\n",
      "Density_Matrix (PredictDensi (None, 4, 4)              0         \n",
      "=================================================================\n",
      "Total params: 5,368,184\n",
      "Trainable params: 5,368,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
